# -*- coding: utf-8 -*-
"""image_captioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NNh1JfHBrdr8hcLFvU2n5s-v1xDYEZQy
"""

from IPython import get_ipython
from IPython.display import display
!pip uninstall torch torchvision torchtext -y -q
!pip uninstall numpy -y -q

# Install a compatible numpy version (e.g., < 2)
!pip install numpy==1.26.3 -q
!pip install torch==2.0 -q
!pip install torchtext==0.15.1 -q
!pip install torchvision==0.15.1 -q


import torch
import torchtext
import torchvision
from torch.utils.data import DataLoader
from torchvision.datasets import Flickr30k
from torchvision import transforms, datasets
torchtext.__version__

device='cuda' if torch.cuda.is_available() else 'cpu'
device
import numpy
numpy.__version__

!pip install -q kaggle
import os
os.environ['KAGGLE_CONFIG_DIR'] = './content'
os.environ['KAGGLEHUB_CACHE'] = './content'

import kagglehub

# Download latest version
path = kagglehub.dataset_download("hsankesara/flickr-image-dataset")

print("Path to dataset files:", path)
if(path=="./content/datasets/hsankesara/flickr-image-dataset/versions/1"):
  !cp -r ./content/datasets/hsankesara/flickr-image-dataset/versions/1 ./content/
else:
  !cp -r /kaggle/input/flickr-image-dataset ./content/

import pandas as pd
label_df= pd.read_csv('/content/content/flickr30k_images/results.csv', sep='|')
label_df.head(10)
#print("DataFrame columns:", label_df.columns)
#label_df[' comment'][1]

!rm -r /content/content/flickr30k_images/flickr30k_images/flickr30k_images
#image_ds = datasets.ImageFolder(root='/content/content/flickr30k_images/',
 #                               transform=transforms.ToTensor())

# from torch.utils.data import random_split
# train_size = int(0.7 * len(image_ds))
# val_size = len(image_ds) - train_size
# train_img_ds, val_img_ds = random_split(image_ds, [train_size, val_size])


# train_img_dl = DataLoader(train_img_ds, batch_size=32, shuffle=True)
# val_img_dl = DataLoader(val_img_ds, batch_size=32, shuffle=False)

import pandas as pd
from pathlib import Path

def build_image_caption_pairs_from_csv(csv_path, images_folder):
    df = pd.read_csv(csv_path, sep='|',engine='python')
    df.columns = df.columns.str.strip()

    df['comment'] = df['comment'].astype(str).fillna('').str.strip()



    # Build (image_path, caption) pairs
    pairs = []
    for _, row in df.iterrows():
        img_path = Path(images_folder) / row['image_name']
        caption = row['comment']
        pairs.append((str(img_path), caption))

    return pairs

pairs = build_image_caption_pairs_from_csv("/content/content/flickr30k_images/results.csv", "/content/content/flickr30k_images/flickr30k_images/")

from torch.utils.data import Dataset
from PIL import Image
class Flickr30kDataset(Dataset):
    def __init__(self, pairs, transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]), tokenizer=None, vocab=None):
        self.pairs = pairs
        self.transform = transform
        self.tokenizer = tokenizer
        self.vocab = vocab

    def __getitem__(self, idx):
        img_path, caption = self.pairs[idx]

        # Load image
        image = Image.open(img_path).convert("RGB")
        if self.transform:
          image = self.transform(image)
        caption = str(caption)

        # Tokenize and convert to indices
        tokens = self.tokenizer(caption)
        caption_ids = [self.vocab["<sos>"]] + self.vocab.lookup_indices(tokens) + [self.vocab["<eos>"]]
        caption_tensor = torch.tensor(caption_ids)

        return image, caption_tensor

    def __len__(self):
        return len(self.pairs)

from torchtext.data.utils import get_tokenizer

MAX_VOCAB_SIZE = 10000
tokenizer = get_tokenizer("basic_english")
tokens = label_df[' comment'].fillna('').map(tokenizer)
from torchtext.vocab import build_vocab_from_iterator
vocab = build_vocab_from_iterator(tokens, specials=["<unk>","<pad>" "<sos>", "<eos>"],max_tokens= MAX_VOCAB_SIZE)
vocab.set_default_index(vocab["<unk>"])

from torch.nn.utils.rnn import pad_sequence

def padding(batch):
    images, captions = zip(*batch)  # unzip batch of (image, caption_tensor)

    images = torch.stack(images)  # shape: [batch_size, 3, H, W]

    # Pad captions to max length in batch
    padded_captions = pad_sequence(captions, batch_first=True, padding_value=1)  # [batch_size, max_len]

    return images, padded_captions

import pandas as pd
import random
shuffle_pairs = random.sample(pairs, len(pairs))

# Define a size for your train set
train_size = int(0.7 * len(pairs))

# Split your dataset
train_pairs = shuffle_pairs[:train_size]
val_pairs = shuffle_pairs[train_size:]

train_dataset = Flickr30kDataset(train_pairs, tokenizer=tokenizer, vocab=vocab)
val_dataset   = Flickr30kDataset(val_pairs, tokenizer=tokenizer, vocab=vocab)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=padding)
val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=padding)
len(train_loader), len(val_loader)

import numpy as np
for image, caption in train_loader:
  print(image.shape, caption.shape)
  break

#build cnn, use pretrained resnet, freeze before final classification layer
#send that output through linear layer
#seNd that to hidden layer of decoder, along with caption as the input.
#use teacher forcing in decoder
#finally send output to train loop and calc loss

from torchvision.models import resnet50
CNN_encoder = resnet50(pretrained=True)
CNN_encoder.fc = torch.nn.Identity()

for image, caption in train_loader:
  print(image.shape)
  output = CNN_encoder(image)
  print(output.shape)
  break

import torch.nn.functional as F
import torch.nn as nn
class LuongAttention(nn.Module):
    def __init__(self, hidden_dim):
        super(LuongAttention, self).__init__()

    def forward(self, decoder_hidden, encoder_outputs):
        decoder_hidden = decoder_hidden.unsqueeze(1)
        attn_scores = torch.bmm(decoder_hidden, encoder_outputs.transpose(1, 2))
        attn_weights = F.softmax(attn_scores, dim=-1)

        context = torch.bmm(attn_weights, encoder_outputs)
        context = context.squeeze(1)

        return context, attn_weights.squeeze(1)

import torch.nn as nn
EMBED_SIZE = 256
HIDDEN_DIM = 32
class CaptionDecoderWithAttention(nn.Module):
    def __init__(self, vocab_size, embed_size=EMBED_SIZE, hidden_dim=HIDDEN_DIM, encoder_dim=2048):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.encoder = CNN_encoder
        self.linear = nn.Linear(encoder_dim, hidden_dim)
        self.embed = nn.Embedding(vocab_size, embed_size)

        self.attention = LuongAttention(hidden_dim)
        self.lstm = nn.LSTMCell(embed_size + hidden_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, images, captions):
        batch_size = images.size(0)
        seq_len = captions.size(1)

        encoder_feats = self.encoder(images)
        encoder_feats = self.linear(encoder_feats)
        encoder_outputs = encoder_feats.unsqueeze(1)

        embeddings = self.embed(captions)

        h, c = torch.zeros(batch_size, self.hidden_dim).to(images.device), \
               torch.zeros(batch_size, self.hidden_dim).to(images.device)

        outputs = []
        for t in range(seq_len):
            context, _ = self.attention(h, encoder_outputs)
            lstm_input = torch.cat([embeddings[:, t], context], dim=-1)
            h, c = self.lstm(lstm_input, (h, c))
            output = self.fc(h)
            outputs.append(output)

        outputs = torch.stack(outputs, dim=1)
        return outputs

final_model = CaptionDecoderWithAttention(embed_size=256,hidden_dim=32,vocab_size=len(vocab)).to(device)
loss_fn = nn.CrossEntropyLoss(ignore_index=vocab["<pad>"])
optimizer = torch.optim.Adam(final_model.parameters(), lr=1e-4)

def train_step(device=device):
  total_loss=0
  final_model.train()
  for image, caption in train_loader:
    image, caption = image.to(device), caption.to(device)

    inputs  = caption[:, :-1]
    targets = caption[:, 1:]
    outputs = final_model(image, inputs)

    outputs = outputs.reshape(-1, outputs.shape[-1])
    targets = targets.reshape(-1)


    optimizer.zero_grad()
    loss = loss_fn(outputs, targets)
    total_loss+=loss.item()
    loss.backward()
    optimizer.step()
  return total_loss/len(train_loader)

epochs=10
for epoch in range(epochs):
  loss = train_step()
  print(f"Epoch: {epoch}, Loss: {loss:.4f}")

def generate_caption(model, image, vocab, max_len=30):
    model.eval()
    with torch.no_grad():
        image = image.unsqueeze(0).to(device)
        encoder_feats = model.encoder(image)
        encoder_feats = model.linear(encoder_feats)      # (1, hidden_dim)
        encoder_outputs = encoder_feats.unsqueeze(1)     # (1, 1, hidden_dim)

        h, c = torch.zeros(1, model.hidden_dim).to(device), torch.zeros(1, model.hidden_dim).to(device)
        word = torch.tensor([vocab["<start>"]]).to(device)
        caption = []

        for _ in range(max_len):
            word_embed = model.embed(word)               # (1, embed_size)
            context, _ = model.attention(h, encoder_outputs)
            lstm_input = torch.cat([word_embed.squeeze(1), context], dim=-1)
            h, c = model.lstm(lstm_input, (h, c))
            output = model.fc(h)                         # (1, vocab_size)
            word = output.argmax(1)

            predicted_word = vocab.itos[word.item()]
            if predicted_word == "<end>":
                break
            caption.append(predicted_word)
        return " ".join(caption)

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
smoothie = SmoothingFunction().method4

def evaluate_bleu(model, data_loader, vocab, num_samples=100):
    model.eval()
    total_bleu = 0
    count = 0

    for images, captions in data_loader:
        images, captions = images.to(device), captions.to(device)
        for i in range(min(images.size(0), num_samples - count)):
            generated = generate_caption(model, images[i], vocab).split()
            reference = [vocab.itos[token.item()] for token in captions[i] if token.item() not in [vocab["<pad>"], vocab["<start>"], vocab["<end>"]]]
            score = sentence_bleu([reference], generated, smoothing_function=smoothie)
            total_bleu += score
            count += 1
            if count >= num_samples:
                break
        if count >= num_samples:
            break

    print(f"BLEU Score on {count} samples: {total_bleu / count:.4f}")
    return total_bleu / count

evaluate_bleu(final_model, val_loader, vocab, num_samples=100)

# import matplotlib.pyplot as plt
# import torch
# import torchvision.transforms as transforms
# from PIL import Image

# def caption_image(model, image_tensor, vocab, max_len=30):
#     model.eval()
#     with torch.no_grad():
#         features = model.encoder(image_tensor.unsqueeze(0).to(device))
#         output_ids = model.decoder.generate_caption(features, max_len=max_len)  # Assume your decoder has a generate_caption method
#         caption = [vocab.idx2word[idx] for idx in output_ids if idx not in {vocab.stoi["<PAD>"], vocab.stoi["<START>"], vocab.stoi["<END>"]}]
#     return " ".join(caption)

# def visualize_predictions(model, test_dataset, vocab, num_samples=5):
#     model.eval()
#     indices = torch.randint(0, len(test_dataset), (num_samples,))
#     for i in indices:
#         img_tensor, caption_ids = test_dataset[i]
#         gt_caption = " ".join([vocab.idx2word[idx] for idx in caption_ids if idx not in {vocab.stoi["<PAD>"], vocab.stoi["<START>"], vocab.stoi["<END>"]}])

#         pred_caption = caption_image(model, img_tensor.to(device), vocab)

#         # Display
#         plt.imshow(transforms.ToPILImage()(img_tensor))
#         plt.title(f"Predicted: {pred_caption}\nGround Truth: {gt_caption}", fontsize=10)
#         plt.axis('off')
#         plt.show()

